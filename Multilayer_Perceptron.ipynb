{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anulekhya/Machine-Learning/blob/main/Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZJE0GriIutA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjp7tT6dJ8pp"
      },
      "source": [
        "- from sklearn.datasets import load_wine: This imports the load_wine function -from scikit-learn's datasets module, which allows us to load the Wine dataset.\n",
        "-from sklearn.model_selection import train_test_split: This imports the train_test_split function from scikit-learn's model_selection module, which helps us split the dataset into training and testing sets.\n",
        "-from sklearn.preprocessing import StandardScaler: This imports the StandardScaler class from scikit-learn's preprocessing module, which is used to standardize the dataset by removing the mean and scaling to unit variance.\n",
        "-import numpy as np: This imports the NumPy library, which provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9l8EgOPIupJ"
      },
      "outputs": [],
      "source": [
        "# Load the wine dataset\n",
        "data = load_wine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz7ZTw0aIulX"
      },
      "outputs": [],
      "source": [
        "#Splitting the dataset\n",
        "#This extracts the features (X) and target labels (y) from the loaded dataset.\n",
        "X, y = data.data, data.target\n",
        "# Split the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ3kheCoLPAY"
      },
      "source": [
        "-This splits the dataset into training and testing sets. Here, 30% of the data is reserved for testing, and random_state=42 sets a seed for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMr46kABIuiS"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTm1p5bDLeBH"
      },
      "source": [
        "- scaler = StandardScaler(): This initializes a StandardScaler object.\n",
        "- X_train = scaler.fit_transform(X_train): This standardizes the training features using the fit_transform method, which computes the mean and standard deviation of each feature and then scales the features.\n",
        "- X_val = scaler.transform(X_val): This standardizes the validation features using the previously computed mean and standard deviation from the training set.\n",
        "- X_test = scaler.transform(X_test): This standardizes the test features using the same mean and standard deviation computed from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPG81lqnIue3"
      },
      "outputs": [],
      "source": [
        "# Defining the model architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\", input_dim=X.shape[1]),  # Hidden layer with 64 neurons and ReLU activation\n",
        "    layers.Dense(32, activation=\"relu\"),                        # Another hidden layer with 32 neurons and ReLU activation\n",
        "    layers.Dense(3, activation=\"softmax\")                       # Output layer with softmax activation for multiclass classification\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVtO_dp4MBeI"
      },
      "source": [
        "- This defines a sequential model using Keras. We define three layers: two hidden layers with 64 and 32 neurons respectively, and an output layer with 3 neurons (one for each class in the dataset) using softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Labs0NFOIuby"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fY5taW4MVqr"
      },
      "source": [
        "- This compiles the model. We specify the Adam optimizer, sparse categorical cross-entropy loss function (suitable for multiclass classification tasks), and accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOBn_NheIuYu",
        "outputId": "5a84715f-bc96-442c-81bb-af6cfb202934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 1s 71ms/step - loss: 1.0968 - accuracy: 0.3710 - val_loss: 1.0044 - val_accuracy: 0.4815\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.9601 - accuracy: 0.5403 - val_loss: 0.8886 - val_accuracy: 0.6296\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.8464 - accuracy: 0.6935 - val_loss: 0.7886 - val_accuracy: 0.7037\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7455 - accuracy: 0.8145 - val_loss: 0.7005 - val_accuracy: 0.8519\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.6626 - accuracy: 0.8952 - val_loss: 0.6208 - val_accuracy: 0.8889\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.5838 - accuracy: 0.9194 - val_loss: 0.5509 - val_accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5118 - accuracy: 0.9274 - val_loss: 0.4881 - val_accuracy: 0.8889\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4486 - accuracy: 0.9435 - val_loss: 0.4317 - val_accuracy: 0.8889\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3906 - accuracy: 0.9758 - val_loss: 0.3823 - val_accuracy: 0.9259\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3401 - accuracy: 0.9839 - val_loss: 0.3395 - val_accuracy: 0.9630\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fba6df6b520>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnmJxwo0MlIf"
      },
      "source": [
        "- This trains the model on the training data for 10 epochs, using the validation data for validation during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irh7yit_IuU6",
        "outputId": "30191b5f-a5c3-422b-b5d9-d4b35f226438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3258 - accuracy: 0.9630\n",
            "Test accuracy: 0.963\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfUUiBqKM9Kb"
      },
      "source": [
        "-  This evaluates the trained model on the test data and computes the test loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OpL0BXLIuRI",
        "outputId": "220eaea5-810c-4927-cec9-16137d90b931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predictions:\n",
            "Sample 1: [1. 0. 0.]\n",
            "Sample 2: [1. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on new data\n",
        "X_new = np.array([\n",
        "    [13.71, 1.86, 2.36, 16.6, 101.2, 2.61, 2.88, 0.27, 1.69, 3.8, 1.11, 4.0, 1035],\n",
        "    [13.56, 1.73, 2.46, 20.5, 116.0, 2.96, 2.78, 0.2, 2.45, 6.25, 0.98, 3.03, 1120]\n",
        "])\n",
        "predictions = model.predict(X_new)\n",
        "print(\"Predictions:\")\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Sample {i+1}: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNHldOiHNLFO"
      },
      "source": [
        "- predictions = model.predict(X_new): This makes predictions on new data (X_new) using the trained model.\n",
        "The print(\"Predictions:\") line prints a header for the predictions.\n",
        "for i, pred in enumerate(predictions): print(f\"Sample {i+1}: {pred}\"): This iterates over the predictions and prints the predicted probabilities for each sample in X_new."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONfiNJy/DY/m4i4u0Owz/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}